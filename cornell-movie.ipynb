{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cornell-movie.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"PgIZrguaRyjJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2622},"outputId":"697f81c6-03f8-4d2f-f970-d0633d74d42f","executionInfo":{"status":"ok","timestamp":1535192017011,"user_tz":-330,"elapsed":137676,"user":{"displayName":"Mohit Saini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"118292376089177322374"}}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Preconfiguring packages ...\n","Selecting previously unselected package cron.\n","(Reading database ... 18408 files and directories currently installed.)\n","Preparing to unpack .../00-cron_3.0pl1-128ubuntu5_amd64.deb ...\n","Unpacking cron (3.0pl1-128ubuntu5) ...\n","Selecting previously unselected package libapparmor1:amd64.\n","Preparing to unpack .../01-libapparmor1_2.11.0-2ubuntu17.1_amd64.deb ...\n","Unpacking libapparmor1:amd64 (2.11.0-2ubuntu17.1) ...\n","Selecting previously unselected package libdbus-1-3:amd64.\n","Preparing to unpack .../02-libdbus-1-3_1.10.22-1ubuntu1_amd64.deb ...\n","Unpacking libdbus-1-3:amd64 (1.10.22-1ubuntu1) ...\n","Selecting previously unselected package dbus.\n","Preparing to unpack .../03-dbus_1.10.22-1ubuntu1_amd64.deb ...\n","Unpacking dbus (1.10.22-1ubuntu1) ...\n","Selecting previously unselected package dirmngr.\n","Preparing to unpack .../04-dirmngr_2.1.15-1ubuntu8.1_amd64.deb ...\n","Unpacking dirmngr (2.1.15-1ubuntu8.1) ...\n","Selecting previously unselected package distro-info-data.\n","Preparing to unpack .../05-distro-info-data_0.36ubuntu0.2_all.deb ...\n","Unpacking distro-info-data (0.36ubuntu0.2) ...\n","Selecting previously unselected package libkmod2:amd64.\n","Preparing to unpack .../06-libkmod2_24-1ubuntu2_amd64.deb ...\n","Unpacking libkmod2:amd64 (24-1ubuntu2) ...\n","Selecting previously unselected package kmod.\n","Preparing to unpack .../07-kmod_24-1ubuntu2_amd64.deb ...\n","Unpacking kmod (24-1ubuntu2) ...\n","Selecting previously unselected package lsb-release.\n","Preparing to unpack .../08-lsb-release_9.20160110ubuntu5_all.deb ...\n","Unpacking lsb-release (9.20160110ubuntu5) ...\n","Selecting previously unselected package libgirepository-1.0-1:amd64.\n","Preparing to unpack .../09-libgirepository-1.0-1_1.54.1-1_amd64.deb ...\n","Unpacking libgirepository-1.0-1:amd64 (1.54.1-1) ...\n","Selecting previously unselected package gir1.2-glib-2.0:amd64.\n","Preparing to unpack .../10-gir1.2-glib-2.0_1.54.1-1_amd64.deb ...\n","Unpacking gir1.2-glib-2.0:amd64 (1.54.1-1) ...\n","Selecting previously unselected package iso-codes.\n","Preparing to unpack .../11-iso-codes_3.75-1_all.deb ...\n","Unpacking iso-codes (3.75-1) ...\n","Selecting previously unselected package libdbus-glib-1-2:amd64.\n","Preparing to unpack .../12-libdbus-glib-1-2_0.108-2_amd64.deb ...\n","Unpacking libdbus-glib-1-2:amd64 (0.108-2) ...\n","Selecting previously unselected package python-apt-common.\n","Preparing to unpack .../13-python-apt-common_1.4.0~beta3build2_all.deb ...\n","Unpacking python-apt-common (1.4.0~beta3build2) ...\n","Selecting previously unselected package python3-apt.\n","Preparing to unpack .../14-python3-apt_1.4.0~beta3build2_amd64.deb ...\n","Unpacking python3-apt (1.4.0~beta3build2) ...\n","Selecting previously unselected package python3-dbus.\n","Preparing to unpack .../15-python3-dbus_1.2.4-1build3_amd64.deb ...\n","Unpacking python3-dbus (1.2.4-1build3) ...\n","Selecting previously unselected package python3-gi.\n","Preparing to unpack .../16-python3-gi_3.24.1-2build1_amd64.deb ...\n","Unpacking python3-gi (3.24.1-2build1) ...\n","Selecting previously unselected package module-init-tools.\n","Preparing to unpack .../17-module-init-tools_24-1ubuntu2_all.deb ...\n","Unpacking module-init-tools (24-1ubuntu2) ...\n","Selecting previously unselected package python-apt.\n","Preparing to unpack .../18-python-apt_1.4.0~beta3build2_amd64.deb ...\n","Unpacking python-apt (1.4.0~beta3build2) ...\n","Selecting previously unselected package python-pycurl.\n","Preparing to unpack .../19-python-pycurl_7.43.0-2build2_amd64.deb ...\n","Unpacking python-pycurl (7.43.0-2build2) ...\n","Selecting previously unselected package python-software-properties.\n","Preparing to unpack .../20-python-software-properties_0.96.24.17_all.deb ...\n","Unpacking python-software-properties (0.96.24.17) ...\n","Selecting previously unselected package python3-software-properties.\n","Preparing to unpack .../21-python3-software-properties_0.96.24.17_all.deb ...\n","Unpacking python3-software-properties (0.96.24.17) ...\n","Selecting previously unselected package software-properties-common.\n","Preparing to unpack .../22-software-properties-common_0.96.24.17_all.deb ...\n","Unpacking software-properties-common (0.96.24.17) ...\n","Selecting previously unselected package unattended-upgrades.\n","Preparing to unpack .../23-unattended-upgrades_0.98ubuntu1.1_all.deb ...\n","Unpacking unattended-upgrades (0.98ubuntu1.1) ...\n","Setting up python-apt-common (1.4.0~beta3build2) ...\n","Setting up python3-apt (1.4.0~beta3build2) ...\n","Setting up iso-codes (3.75-1) ...\n","Setting up distro-info-data (0.36ubuntu0.2) ...\n","Setting up python-pycurl (7.43.0-2build2) ...\n","Setting up lsb-release (9.20160110ubuntu5) ...\n","Setting up libgirepository-1.0-1:amd64 (1.54.1-1) ...\n","Setting up libkmod2:amd64 (24-1ubuntu2) ...\n","Setting up gir1.2-glib-2.0:amd64 (1.54.1-1) ...\n","Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n","Setting up libapparmor1:amd64 (2.11.0-2ubuntu17.1) ...\n","Setting up unattended-upgrades (0.98ubuntu1.1) ...\n","\n","Creating config file /etc/apt/apt.conf.d/20auto-upgrades with new version\n","\n","Creating config file /etc/apt/apt.conf.d/50unattended-upgrades with new version\n","invoke-rc.d: could not determine current runlevel\n","invoke-rc.d: policy-rc.d denied execution of start.\n","Setting up dirmngr (2.1.15-1ubuntu8.1) ...\n","Setting up cron (3.0pl1-128ubuntu5) ...\n","Adding group `crontab' (GID 102) ...\n","Done.\n","update-rc.d: warning: start and stop actions are no longer supported; falling back to defaults\n","update-rc.d: warning: stop runlevel arguments (1) do not match cron Default-Stop values (none)\n","invoke-rc.d: could not determine current runlevel\n","invoke-rc.d: policy-rc.d denied execution of start.\n","Setting up libdbus-1-3:amd64 (1.10.22-1ubuntu1) ...\n","Setting up kmod (24-1ubuntu2) ...\n","Setting up libdbus-glib-1-2:amd64 (0.108-2) ...\n","Setting up python3-gi (3.24.1-2build1) ...\n","Setting up module-init-tools (24-1ubuntu2) ...\n","Setting up python3-software-properties (0.96.24.17) ...\n","Setting up dbus (1.10.22-1ubuntu1) ...\n","Setting up python-apt (1.4.0~beta3build2) ...\n","Setting up python3-dbus (1.2.4-1build3) ...\n","Setting up python-software-properties (0.96.24.17) ...\n","Setting up software-properties-common (0.96.24.17) ...\n","Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n","Processing triggers for dbus (1.10.22-1ubuntu1) ...\n","gpg: keybox '/tmp/tmpeuy0yl8f/pubring.gpg' created\n","gpg: /tmp/tmpeuy0yl8f/trustdb.gpg: trustdb created\n","gpg: key AD5F235DF639B041: public key \"Launchpad PPA for Alessandro Strada\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n","Selecting previously unselected package libfuse2:amd64.\n","(Reading database ... 19816 files and directories currently installed.)\n","Preparing to unpack .../libfuse2_2.9.7-1ubuntu1_amd64.deb ...\n","Unpacking libfuse2:amd64 (2.9.7-1ubuntu1) ...\n","Selecting previously unselected package fuse.\n","Preparing to unpack .../fuse_2.9.7-1ubuntu1_amd64.deb ...\n","Unpacking fuse (2.9.7-1ubuntu1) ...\n","Selecting previously unselected package google-drive-ocamlfuse.\n","Preparing to unpack .../google-drive-ocamlfuse_0.6.21-0ubuntu2_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.6.21-0ubuntu2) ...\n","Setting up libfuse2:amd64 (2.9.7-1ubuntu1) ...\n","Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n","Setting up fuse (2.9.7-1ubuntu1) ...\n","Setting up google-drive-ocamlfuse (0.6.21-0ubuntu2) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"metadata":{"id":"Mk9YvfYrRzR4","colab_type":"code","colab":{}},"cell_type":"code","source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yS-k2XnXS6ad","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1687},"outputId":"f8fc9047-91bb-48dc-af99-a2fb3852eca4","executionInfo":{"status":"error","timestamp":1535192918549,"user_tz":-330,"elapsed":80239,"user":{"displayName":"Mohit Saini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"118292376089177322374"}}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import pickle\n","import datetime\n","\n","def load_preprocess():\n","    with open('drive/preprocess.p', mode='rb') as in_file:\n","        return pickle.load(in_file)\n","\n","\n","(source_int_text, target_int_text), (source_int_to_vocab, target_int_to_vocab) , (source_vocab_to_int, target_vocab_to_int) = load_preprocess()\n","\n","# =============================================================================\n","#       STEPS INVOLVED:\n","\n","#     (1) define input parameters to the encoder model\n","#         enc_dec_model_inputs\n","#     (2) build encoder model\n","#         encoding_layer\n","#     (3) define input parameters to the decoder model\n","#         enc_dec_model_inputs, process_decoder_input, decoding_layer\n","#     (4) build decoder model for training\n","#         decoding_layer_train\n","#     (5) build decoder model for inference\n","#         decoding_layer_infer\n","#     (6) put (4) and (5) together\n","#         decoding_layer\n","#     (7) connect encoder and decoder models\n","#         seq2seq_model\n","#     (8) train and estimate loss and accuracy\n","# \n","# =============================================================================\n","\n","def enc_dec_model_inputs():\n","    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n","    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n","    \n","    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n","    source_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n","    max_target_len = tf.reduce_max(target_sequence_length)    \n","    \n","    return inputs, targets, target_sequence_length, max_target_len, source_sequence_length\n","\n","def hyperparam_inputs():\n","    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n","    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n","    \n","    return lr_rate, keep_prob\n","\n","def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n","    \"\"\"\n","    Preprocess target data for encoding\n","    :return: Preprocessed target data\n","    \"\"\"\n","    # get '<GO>' id\n","    go_id = target_vocab_to_int['<GO>']\n","    \n","    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n","    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n","    \n","    return after_concat\n","\n","\n","def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n","                   source_vocab_size, \n","                   encoding_embedding_size,\n","                   source_sequence_length):\n","    \"\"\"\n","    :return: tuple (RNN output, RNN state)\n","    \"\"\"\n","    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n","                                             vocab_size=source_vocab_size, \n","                                             embed_dim=encoding_embedding_size)\n","    \n","    stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n","    \n","    outputs, state = tf.nn.bidirectional_dynamic_rnn(cell_fw=stacked_cells, \n","                                                             cell_bw=stacked_cells, \n","                                                             inputs=embed, \n","                                                             sequence_length=source_sequence_length, \n","                                                             dtype=tf.float32)\n","    \n","    concat_outputs = tf.concat(outputs, 2)\n","    return concat_outputs, state\n","\n","def decoding_layer_train(encoder_outputs, encoder_state, dec_cell, dec_embed_input, \n","                         target_sequence_length, max_summary_length, \n","                         output_layer, keep_prob):\n","    \"\"\"\n","    Create a training process in decoding layer \n","    :return: BasicDecoderOutput containing training logits and sample_id\n","    \"\"\"\n","    \n","    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n","                                             output_keep_prob=keep_prob)\n","    \n","    \n","    train_helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, target_sequence_length)\n","    \n","    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(rnn_size, encoder_outputs,\n","                                                               memory_sequence_length=target_sequence_length)\n","    \n","    attention_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism,\n","                                                         attention_layer_size=rnn_size/2)\n","    \n","    decoder = tf.contrib.seq2seq.BasicDecoder(cell=attention_cell, helper=train_helper, \n","                                              initial_state=attention_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n","                                              output_layer=output_layer) \n","    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations=max_summary_length)\n","    \n","    return outputs\n","\n","\n","\n","def decoding_layer_infer(encoder_outputs, encoder_state, dec_cell,\n","                         dec_embeddings, start_of_sequence_id,\n","                         end_of_sequence_id, max_target_sequence_length,\n","                         vocab_size, output_layer, batch_size, keep_prob,\n","                         target_sequence_length):\n","    \"\"\"\n","    Create a inference process in decoding layer \n","    :return: BasicDecoderOutput containing inference logits and sample_id\n","    \"\"\"\n","    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n","                                             output_keep_prob=keep_prob)\n","    \n","    infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n","                                                      tf.fill([batch_size], start_of_sequence_id), \n","                                                      end_of_sequence_id)\n","    \n","    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(rnn_size, encoder_outputs,\n","                                                               memory_sequence_length=target_sequence_length)\n","    \n","    attention_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism,\n","                                                         attention_layer_size=rnn_size/2)\n","    \n","    decoder = tf.contrib.seq2seq.BasicDecoder(cell=attention_cell, helper=infer_helper, \n","                                              initial_state=attention_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n","                                              output_layer=output_layer)\n","   \n","    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations=max_target_sequence_length)\n","    \n","    return outputs\n","\n","def decoding_layer(encoder_outputs, dec_input, encoder_state,\n","                   target_sequence_length, max_target_sequence_length,\n","                   rnn_size,\n","                   num_layers, target_vocab_to_int, target_vocab_size,\n","                   batch_size, keep_prob, decoding_embedding_size):\n","    \"\"\"\n","    Create decoding layer\n","    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n","    \"\"\"\n","    target_vocab_size = len(target_vocab_to_int) + 1\n","    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n","    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n","    \n","    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n","    \n","    with tf.variable_scope(\"decode\"):\n","        output_layer = tf.layers.Dense(target_vocab_size)\n","        train_output = decoding_layer_train(encoder_outputs,\n","                                            encoder_state, \n","                                            cells, \n","                                            dec_embed_input, \n","                                            target_sequence_length, \n","                                            max_target_sequence_length, \n","                                            output_layer, \n","                                            keep_prob)\n","\n","    with tf.variable_scope(\"decode\", reuse=True):\n","        infer_output = decoding_layer_infer(encoder_outputs,\n","                                            encoder_state, \n","                                            cells, \n","                                            dec_embeddings, \n","                                            target_vocab_to_int['<GO>'], \n","                                            target_vocab_to_int['<EOS>'], \n","                                            max_target_sequence_length, \n","                                            target_vocab_size, \n","                                            output_layer,\n","                                            batch_size,\n","                                            keep_prob,\n","                                            target_sequence_length)\n","\n","    return (train_output, infer_output)\n","\n","def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n","                  target_sequence_length,\n","                  max_target_sentence_length,\n","                  source_vocab_size, target_vocab_size,\n","                  enc_embedding_size, dec_embedding_size,\n","                  rnn_size, num_layers, target_vocab_to_int,\n","                  source_sequence_length):\n","    \"\"\"\n","    Build the Sequence-to-Sequence model\n","    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n","    \"\"\"\n","    enc_outputs, enc_states = encoding_layer(input_data, \n","                                             rnn_size, \n","                                             num_layers, \n","                                             keep_prob, \n","                                             source_vocab_size, \n","                                             enc_embedding_size,\n","                                             source_sequence_length)\n","    \n","    dec_input = process_decoder_input(target_data, \n","                                      target_vocab_to_int, \n","                                      batch_size)\n","    \n","    train_output, infer_output = decoding_layer(enc_outputs,\n","                                                dec_input,\n","                                               enc_states, \n","                                               target_sequence_length, \n","                                               max_target_sentence_length,\n","                                               rnn_size,\n","                                              num_layers,\n","                                              target_vocab_to_int,\n","                                              target_vocab_size,\n","                                              batch_size,\n","                                              keep_prob,\n","                                              dec_embedding_size)\n","    \n","    return train_output, infer_output\n","\n","\n","display_step = 30\n","\n","epochs = 20\n","batch_size = 128\n","\n","rnn_size = 128\n","num_layers = 3\n","\n","encoding_embedding_size = 200\n","decoding_embedding_size = 200\n","\n","learning_rate = 0.001\n","learning_rate_decay = 0.9\n","min_learning_rate = 0.0001\n","keep_probability = 0.5\n","\n","\n","\n","save_path = 'checkpoints/dev'\n","(source_int_text, target_int_text), _, (source_vocab_to_int, target_vocab_to_int) = load_preprocess()\n","max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n","\n","train_graph = tf.Graph()\n","with train_graph.as_default():\n","    input_data, targets, target_sequence_length, max_target_sequence_length, source_sequence_length = enc_dec_model_inputs()\n","    lr, keep_prob = hyperparam_inputs()\n","    \n","    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n","                                                   targets,\n","                                                   keep_prob,\n","                                                   batch_size,\n","                                                   target_sequence_length,\n","                                                   max_target_sequence_length,\n","                                                   len(source_vocab_to_int),\n","                                                   len(target_vocab_to_int),\n","                                                   encoding_embedding_size,\n","                                                   decoding_embedding_size,\n","                                                   rnn_size,\n","                                                   num_layers,\n","                                                   target_vocab_to_int,\n","                                                   source_sequence_length)\n","    \n","    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n","    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n","\n","    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n","    # - Returns a mask tensor representing the first N positions of each cell.\n","    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n","    \n","    \n","\n","    with tf.name_scope(\"optimization\"):\n","        # Loss function - weighted softmax cross entropy\n","        cost = tf.contrib.seq2seq.sequence_loss(\n","            training_logits,\n","            targets,\n","            masks)\n","\n","        # Optimizer\n","        optimizer = tf.train.AdamOptimizer(lr)\n","\n","        # Gradient Clipping\n","        gradients = optimizer.compute_gradients(cost)\n","        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n","        train_op = optimizer.apply_gradients(capped_gradients)\n","        \n","        tf.summary.scalar('loss', cost)\n","        merged = tf.summary.merge_all()\n","        logdir = 'tensorboard/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\" \n","        \n","        \n","def pad_sentence_batch(sentence_batch, pad_int):\n","    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n","    max_sentence = max([len(sentence) for sentence in sentence_batch])\n","    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n","\n","\n","def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n","    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n","    for batch_i in range(0, len(sources)//batch_size):\n","        start_i = batch_i * batch_size\n","\n","        # Slice the right amount for the batch\n","        sources_batch = sources[start_i:start_i + batch_size]\n","        targets_batch = targets[start_i:start_i + batch_size]\n","\n","        # Pad\n","        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n","        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n","\n","        # Need the lengths for the _lengths parameters\n","        pad_targets_lengths = []\n","        for target in pad_targets_batch:\n","            pad_targets_lengths.append(len(target))\n","\n","        pad_source_lengths = []\n","        for source in pad_sources_batch:\n","            pad_source_lengths.append(len(source))\n","\n","        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n","        \n","        \n","def get_accuracy(target, logits):\n","    \"\"\"\n","    Calculate accuracy\n","    \"\"\"\n","    max_seq = max(target.shape[1], logits.shape[1])\n","    if max_seq - target.shape[1]:\n","        target = np.pad(\n","            target,\n","            [(0,0),(0,max_seq - target.shape[1])],\n","            'constant')\n","    if max_seq - logits.shape[1]:\n","        logits = np.pad(\n","            logits,\n","            [(0,0),(0,max_seq - logits.shape[1])],\n","            'constant')\n","\n","    return np.mean(np.equal(target, logits))\n","\n","\n","\n","def sentence_to_seq(sentence, vocab_to_int):\n","    results = []\n","    for word in sentence.split(\" \"):\n","        if word in vocab_to_int:\n","            results.append(vocab_to_int[word])\n","        else:\n","            results.append(vocab_to_int['<UNK>'])\n","            \n","    return results\n","\n","sentances = [\"how are you\",\n","             \"what are you doing\",\n","             \"get up\",\n","             \"go there\",\n","             \"want some food\"]\n","\n","\n","# Split data to training and validation sets\n","train_source = source_int_text[batch_size:]\n","train_target = target_int_text[batch_size:]\n","valid_source = source_int_text[:batch_size]\n","valid_target = target_int_text[:batch_size]\n","(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n","                                                                                                             valid_target,\n","                                                                                                             batch_size,\n","                                                                                                             source_vocab_to_int['<PAD>'],\n","                                                                                                             target_vocab_to_int['<PAD>']))\n","\n","\n","\n","for i in range(6, 20):\n","  with tf.Session(graph=train_graph) as sess:\n","      writer = tf.summary.FileWriter(logdir, train_graph)\n","      saver = tf.train.Saver()\n","      try:\n","          saver.restore(sess, tf.train.latest_checkpoint('drive/checkpoints/'))\n","          print(\"Saved model found\")\n","      except ValueError:\n","          print(\"No saved models found, initializing new variables\")\n","          sess.run(tf.global_variables_initializer())\n","\n","      for epoch_i in range(epochs):\n","          #learning_rate = initial_rate * np.exp(-(i*epoch_i*0.1))\n","          for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n","                  get_batches(train_source, train_target, batch_size,\n","                              source_vocab_to_int['<PAD>'],\n","                              target_vocab_to_int['<PAD>'])):\n","\n","              _, loss = sess.run(\n","                  [train_op, cost],\n","                  {input_data: source_batch,\n","                   targets: target_batch,\n","                   lr: learning_rate,\n","                   target_sequence_length: targets_lengths,\n","                   keep_prob: keep_probability,\n","                   source_sequence_length: sources_lengths})\n","\n","\n","              if batch_i % display_step == 0 and batch_i > 0:\n","                  batch_train_logits = sess.run(\n","                      inference_logits,\n","                      {input_data: source_batch,\n","                       target_sequence_length: targets_lengths,\n","                       keep_prob: 1.0,\n","                       source_sequence_length: sources_lengths})\n","\n","                  batch_valid_logits = sess.run(\n","                      inference_logits,\n","                      {input_data: valid_sources_batch,\n","                       target_sequence_length: valid_targets_lengths,\n","                       keep_prob: 1.0,\n","                       source_sequence_length: valid_sources_lengths})\n","\n","                  train_acc = get_accuracy(target_batch, batch_train_logits)\n","                  valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)       \n","\n","\n","                  print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n","                        .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n","\n","\n","              if batch_i%90 == 0 and batch_i > 0:\n","                  idx = np.random.randint(0,5)\n","                  st = sentence_to_seq(sentances[idx], source_vocab_to_int)\n","\n","                  trans_logits = sess.run(inference_logits, feed_dict={input_data: [st]*batch_size,\n","                                           target_sequence_length: [len(st)*2]*batch_size,\n","                                           source_sequence_length: [len(st)*2]*batch_size,\n","                                           keep_prob: 1.0})[0]\n","                  cst = sess.run(merged,{input_data: source_batch,\n","                                       targets: target_batch,\n","                                       lr: learning_rate,\n","                                       target_sequence_length: targets_lengths,\n","                                       source_sequence_length: sources_lengths,\n","                                       keep_prob: keep_probability})\n","                  writer.add_summary(cst, batch_i)                \n","\n","                  print('Input')\n","                  print('  Word Ids:      {}'.format([i for i in st]))\n","                  print('  input : {}'.format([source_int_to_vocab[i] for i in st]))\n","\n","                  print('\\nPrediction')\n","                  print('  Word Ids:      {}'.format([i for i in trans_logits]))\n","                  print('  reply: {}'.format(\" \".join([target_int_to_vocab[i] for i in trans_logits])))\n","          learning_rate *= learning_rate_decay\n","          if learning_rate < min_learning_rate:\n","              learning_rate = min_learning_rate\n","\n","      # Save Model\n","      saver.save(sess, 'drive/checkpoints/dev',i)\n","      print('Model Trained and Saved')\n","\n","\n","    \n","    \n","\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from drive/checkpoints/dev-13\n","Saved model found\n","Epoch   0 Batch   30/1080 - Train Accuracy: 0.0156, Validation Accuracy: 0.0141, Loss: 1.4998\n","Epoch   0 Batch   60/1080 - Train Accuracy: 0.0167, Validation Accuracy: 0.0134, Loss: 1.5318\n","Epoch   0 Batch   90/1080 - Train Accuracy: 0.0216, Validation Accuracy: 0.0138, Loss: 1.6162\n","Input\n","  Word Ids:      [301, 3249, 2712]\n","  input : ['how', 'are', 'you']\n","\n","Prediction\n","  Word Ids:      [7084, 822, 4319, 523, 5856, 48]\n","  reply: i am not going to be\n","Epoch   0 Batch  120/1080 - Train Accuracy: 0.0238, Validation Accuracy: 0.0153, Loss: 1.4077\n","Epoch   0 Batch  150/1080 - Train Accuracy: 0.0231, Validation Accuracy: 0.0134, Loss: 1.3217\n","Epoch   0 Batch  180/1080 - Train Accuracy: 0.0201, Validation Accuracy: 0.0130, Loss: 1.3050\n","Input\n","  Word Ids:      [2596, 3249, 2712, 996]\n","  input : ['what', 'are', 'you', 'doing']\n","\n","Prediction\n","  Word Ids:      [7084, 822, 4319, 523, 5856, 6703, 5856, 7240]\n","  reply: i am not going to get to it\n","Epoch   0 Batch  210/1080 - Train Accuracy: 0.0197, Validation Accuracy: 0.0141, Loss: 1.4436\n","Epoch   0 Batch  240/1080 - Train Accuracy: 0.0171, Validation Accuracy: 0.0138, Loss: 1.2772\n","Epoch   0 Batch  270/1080 - Train Accuracy: 0.0197, Validation Accuracy: 0.0130, Loss: 1.4234\n","Input\n","  Word Ids:      [6703, 7569]\n","  input : ['get', 'up']\n","\n","Prediction\n","  Word Ids:      [7084, 822, 1020, 8094]\n","  reply: i am sorry <EOS>\n","Epoch   0 Batch  300/1080 - Train Accuracy: 0.0130, Validation Accuracy: 0.0134, Loss: 1.5831\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-450211787c82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    401\u001b[0m                    \u001b[0mtarget_sequence_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargets_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                    \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                    source_sequence_length: sources_lengths})\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"xgYONHmTtNR1","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"t_zVG13ptNV6","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"hfQ7LFKktNw5","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"oUDV6pFytNP2","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"LPwwB9jObcGk","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"1fUSEhb2ar9c","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1969},"outputId":"2f78adc6-4dc7-4c65-fa60-83bd84ee83c2","executionInfo":{"status":"error","timestamp":1535193156444,"user_tz":-330,"elapsed":61405,"user":{"displayName":"Mohit Saini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"118292376089177322374"}}},"cell_type":"code","source":["with tf.Session(graph=train_graph) as sess:\n","  saver = tf.train.Saver()\n","  saver.restore(sess, tf.train.latest_checkpoint('drive/checkpoints/'))\n","  while True:\n","    inp = input()\n","    st = sentence_to_seq(inp, source_vocab_to_int)\n","    trans_logits = sess.run(inference_logits, feed_dict={input_data: [st]*batch_size,\n","                                           target_sequence_length: [len(st)*2]*batch_size,\n","                                           source_sequence_length: [len(st)*2]*batch_size,\n","                                           keep_prob: 1.0})[0]\n","    print('Input')\n","    print('  Word Ids:      {}'.format([i for i in st]))\n","    print('  input : {}'.format([source_int_to_vocab[i] for i in st]))\n","\n","    print('\\nPrediction')\n","    print('  Word Ids:      {}'.format([i for i in trans_logits]))\n","    print('  reply: {}'.format(\" \".join([target_int_to_vocab[i] for i in trans_logits])))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from drive/checkpoints/dev-13\n","what are you doing\n","Input\n","  Word Ids:      [2596, 3249, 2712, 996]\n","  input : ['what', 'are', 'you', 'doing']\n","\n","Prediction\n","  Word Ids:      [7084, 822, 4361, 4747, 5856, 6703, 5856, 6240]\n","  reply: i am just trying to get to the\n","what \n","Input\n","  Word Ids:      [2596, 8095]\n","  input : ['what', '<UNK>']\n","\n","Prediction\n","  Word Ids:      [7084, 822, 1020, 8094]\n","  reply: i am sorry <EOS>\n","what\n","Input\n","  Word Ids:      [2596]\n","  input : ['what']\n","\n","Prediction\n","  Word Ids:      [2362, 4643]\n","  reply: whew typical\n","who are you\n","Input\n","  Word Ids:      [5336, 3249, 2712]\n","  input : ['who', 'are', 'you']\n","\n","Prediction\n","  Word Ids:      [7084, 822, 8095, 8094, 0, 0]\n","  reply: i am <UNK> <EOS> greek greek\n","oh \n","Input\n","  Word Ids:      [6095, 8095]\n","  input : ['oh', '<UNK>']\n","\n","Prediction\n","  Word Ids:      [7084, 822, 1020, 8094]\n","  reply: i am sorry <EOS>\n","\n","Input\n","  Word Ids:      [8095]\n","  input : ['<UNK>']\n","\n","Prediction\n","  Word Ids:      [2362, 4643]\n","  reply: whew typical\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \"\"\"\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-09e0e07a510b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/checkpoints/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_vocab_to_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     trans_logits = sess.run(inference_logits, feed_dict={input_data: [st]*batch_size,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"fpq9IxzzbO2u","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","os.li"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0ocqh-fvhe5Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1365},"outputId":"51c27c17-6d65-4871-a782-1d0cd66ae1c3","executionInfo":{"status":"error","timestamp":1535193351916,"user_tz":-330,"elapsed":27563,"user":{"displayName":"Mohit Saini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"118292376089177322374"}}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import pickle\n","import datetime\n","\n","def load_preprocess():\n","    with open('drive/preprocess.p', mode='rb') as in_file:\n","        return pickle.load(in_file)\n","\n","\n","(source_int_text, target_int_text), (source_int_to_vocab, target_int_to_vocab) , (source_vocab_to_int, target_vocab_to_int) = load_preprocess()\n","\n","\n","\n","def enc_dec_model_inputs():\n","    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n","    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n","    \n","    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n","    source_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n","    max_target_len = tf.reduce_max(target_sequence_length)    \n","    \n","    return inputs, targets, target_sequence_length, max_target_len, source_sequence_length\n","\n","def hyperparam_inputs():\n","    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n","    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n","    \n","    return lr_rate, keep_prob\n","\n","def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n","    \"\"\"\n","    Preprocess target data for encoding\n","    :return: Preprocessed target data\n","    \"\"\"\n","    # get '<GO>' id\n","    go_id = target_vocab_to_int['<GO>']\n","    \n","    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n","    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n","    \n","    return after_concat\n","\n","\n","def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n","                   source_vocab_size, \n","                   encoding_embedding_size,\n","                   source_sequence_length):\n","    \"\"\"\n","    :return: tuple (RNN output, RNN state)\n","    \"\"\"\n","    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n","                                             vocab_size=source_vocab_size, \n","                                             embed_dim=encoding_embedding_size)\n","    \n","    stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n","    \n","    outputs, state = tf.nn.bidirectional_dynamic_rnn(cell_fw=stacked_cells, \n","                                                             cell_bw=stacked_cells, \n","                                                             inputs=embed, \n","                                                             sequence_length=source_sequence_length, \n","                                                             dtype=tf.float32)\n","    \n","    concat_outputs = tf.concat(outputs, 2)\n","    return concat_outputs, state\n","\n","def decoding_layer_train(encoder_outputs, encoder_state, dec_cell, dec_embed_input, \n","                         target_sequence_length, max_summary_length, \n","                         output_layer, keep_prob):\n","    \"\"\"\n","    Create a training process in decoding layer \n","    :return: BasicDecoderOutput containing training logits and sample_id\n","    \"\"\"\n","    \n","    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n","                                             output_keep_prob=keep_prob)\n","    \n","    \n","    train_helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, target_sequence_length)\n","    \n","    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(rnn_size, encoder_outputs,\n","                                                               memory_sequence_length=target_sequence_length)\n","    \n","    attention_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism,\n","                                                         attention_layer_size=rnn_size/2)\n","    \n","    decoder = tf.contrib.seq2seq.BasicDecoder(cell=attention_cell, helper=train_helper, \n","                                              initial_state=attention_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n","                                              output_layer=output_layer) \n","    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations=max_summary_length)\n","    \n","    return outputs\n","\n","\n","\n","def decoding_layer_infer(encoder_outputs, encoder_state, dec_cell,\n","                         dec_embeddings, start_of_sequence_id,\n","                         end_of_sequence_id, max_target_sequence_length,\n","                         vocab_size, output_layer, batch_size, keep_prob,\n","                         target_sequence_length):\n","    \"\"\"\n","    Create a inference process in decoding layer \n","    :return: BasicDecoderOutput containing inference logits and sample_id\n","    \"\"\"\n","    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n","                                             output_keep_prob=keep_prob)\n","    \n","    infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n","                                                      tf.fill([batch_size], start_of_sequence_id), \n","                                                      end_of_sequence_id)\n","    \n","    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(rnn_size, encoder_outputs,\n","                                                               memory_sequence_length=target_sequence_length)\n","    \n","    attention_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism,\n","                                                         attention_layer_size=rnn_size/2)\n","    \n","    decoder = tf.contrib.seq2seq.BasicDecoder(cell=attention_cell, helper=infer_helper, \n","                                              initial_state=attention_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n","                                              output_layer=output_layer)\n","   \n","    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations=max_target_sequence_length)\n","    \n","    return outputs\n","\n","def decoding_layer(encoder_outputs, dec_input, encoder_state,\n","                   target_sequence_length, max_target_sequence_length,\n","                   rnn_size,\n","                   num_layers, target_vocab_to_int, target_vocab_size,\n","                   batch_size, keep_prob, decoding_embedding_size):\n","    \"\"\"\n","    Create decoding layer\n","    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n","    \"\"\"\n","    target_vocab_size = len(target_vocab_to_int) + 1\n","    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n","    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n","    \n","    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n","    \n","    with tf.variable_scope(\"decode\"):\n","        output_layer = tf.layers.Dense(target_vocab_size)\n","        train_output = decoding_layer_train(encoder_outputs,\n","                                            encoder_state, \n","                                            cells, \n","                                            dec_embed_input, \n","                                            target_sequence_length, \n","                                            max_target_sequence_length, \n","                                            output_layer, \n","                                            keep_prob)\n","\n","    with tf.variable_scope(\"decode\", reuse=True):\n","        infer_output = decoding_layer_infer(encoder_outputs,\n","                                            encoder_state, \n","                                            cells, \n","                                            dec_embeddings, \n","                                            target_vocab_to_int['<GO>'], \n","                                            target_vocab_to_int['<EOS>'], \n","                                            max_target_sequence_length, \n","                                            target_vocab_size, \n","                                            output_layer,\n","                                            batch_size,\n","                                            keep_prob,\n","                                            target_sequence_length)\n","\n","    return (train_output, infer_output)\n","\n","def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n","                  target_sequence_length,\n","                  max_target_sentence_length,\n","                  source_vocab_size, target_vocab_size,\n","                  enc_embedding_size, dec_embedding_size,\n","                  rnn_size, num_layers, target_vocab_to_int,\n","                  source_sequence_length):\n","    \"\"\"\n","    Build the Sequence-to-Sequence model\n","    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n","    \"\"\"\n","    enc_outputs, enc_states = encoding_layer(input_data, \n","                                             rnn_size, \n","                                             num_layers, \n","                                             keep_prob, \n","                                             source_vocab_size, \n","                                             enc_embedding_size,\n","                                             source_sequence_length)\n","    \n","    dec_input = process_decoder_input(target_data, \n","                                      target_vocab_to_int, \n","                                      batch_size)\n","    \n","    train_output, infer_output = decoding_layer(enc_outputs,\n","                                                dec_input,\n","                                               enc_states, \n","                                               target_sequence_length, \n","                                               max_target_sentence_length,\n","                                               rnn_size,\n","                                              num_layers,\n","                                              target_vocab_to_int,\n","                                              target_vocab_size,\n","                                              batch_size,\n","                                              keep_prob,\n","                                              dec_embedding_size)\n","    \n","    return train_output, infer_output\n","\n","\n","display_step = 30\n","\n","epochs = 20\n","batch_size = 128\n","\n","rnn_size = 128\n","num_layers = 3\n","\n","encoding_embedding_size = 200\n","decoding_embedding_size = 200\n","\n","learning_rate = 0.001\n","learning_rate_decay = 0.9\n","min_learning_rate = 0.0001\n","keep_probability = 0.5\n","\n","\n","\n","save_path = 'checkpoints/dev'\n","(source_int_text, target_int_text), _, (source_vocab_to_int, target_vocab_to_int) = load_preprocess()\n","max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n","\n","train_graph = tf.Graph()\n","with train_graph.as_default():\n","    input_data, targets, target_sequence_length, max_target_sequence_length, source_sequence_length = enc_dec_model_inputs()\n","    lr, keep_prob = hyperparam_inputs()\n","    \n","    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n","                                                   targets,\n","                                                   keep_prob,\n","                                                   batch_size,\n","                                                   target_sequence_length,\n","                                                   max_target_sequence_length,\n","                                                   len(source_vocab_to_int),\n","                                                   len(target_vocab_to_int),\n","                                                   encoding_embedding_size,\n","                                                   decoding_embedding_size,\n","                                                   rnn_size,\n","                                                   num_layers,\n","                                                   target_vocab_to_int,\n","                                                   source_sequence_length)\n","    \n","    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n","    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n","\n","    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n","    # - Returns a mask tensor representing the first N positions of each cell.\n","    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n","    \n","    \n","\n","    with tf.name_scope(\"optimization\"):\n","        # Loss function - weighted softmax cross entropy\n","        cost = tf.contrib.seq2seq.sequence_loss(\n","            training_logits,\n","            targets,\n","            masks)\n","\n","        # Optimizer\n","        optimizer = tf.train.AdamOptimizer(lr)\n","\n","        # Gradient Clipping\n","        gradients = optimizer.compute_gradients(cost)\n","        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n","        train_op = optimizer.apply_gradients(capped_gradients)\n","        \n","        tf.summary.scalar('loss', cost)\n","        merged = tf.summary.merge_all()\n","        logdir = 'tensorboard/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\" \n","        \n","        \n","def pad_sentence_batch(sentence_batch, pad_int):\n","    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n","    max_sentence = max([len(sentence) for sentence in sentence_batch])\n","    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n","\n","\n","def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n","    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n","    for batch_i in range(0, len(sources)//batch_size):\n","        start_i = batch_i * batch_size\n","\n","        # Slice the right amount for the batch\n","        sources_batch = sources[start_i:start_i + batch_size]\n","        targets_batch = targets[start_i:start_i + batch_size]\n","\n","        # Pad\n","        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n","        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n","\n","        # Need the lengths for the _lengths parameters\n","        pad_targets_lengths = []\n","        for target in pad_targets_batch:\n","            pad_targets_lengths.append(len(target))\n","\n","        pad_source_lengths = []\n","        for source in pad_sources_batch:\n","            pad_source_lengths.append(len(source))\n","\n","        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n","        \n","        \n","def get_accuracy(target, logits):\n","    \"\"\"\n","    Calculate accuracy\n","    \"\"\"\n","    max_seq = max(target.shape[1], logits.shape[1])\n","    if max_seq - target.shape[1]:\n","        target = np.pad(\n","            target,\n","            [(0,0),(0,max_seq - target.shape[1])],\n","            'constant')\n","    if max_seq - logits.shape[1]:\n","        logits = np.pad(\n","            logits,\n","            [(0,0),(0,max_seq - logits.shape[1])],\n","            'constant')\n","\n","    return np.mean(np.equal(target, logits))\n","\n","\n","\n","def sentence_to_seq(sentence, vocab_to_int):\n","    results = []\n","    for word in sentence.split(\" \"):\n","        if word in vocab_to_int:\n","            results.append(vocab_to_int[word])\n","        else:\n","            results.append(vocab_to_int['<UNK>'])\n","            \n","    return results\n","\n","sentances = [\"how are you\",\n","             \"what are you doing\",\n","             \"get up\",\n","             \"go there\",\n","             \"want some food\"]\n","\n","\n","# Split data to training and validation sets\n","train_source = source_int_text[batch_size:]\n","train_target = target_int_text[batch_size:]\n","valid_source = source_int_text[:batch_size]\n","valid_target = target_int_text[:batch_size]\n","(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n","                                                                                                             valid_target,\n","                                                                                                             batch_size,\n","                                                                                                             source_vocab_to_int['<PAD>'],\n","                                                                                                             target_vocab_to_int['<PAD>']))\n","\n","\n","with tf.Session(graph=train_graph) as sess:\n","  saver = tf.train.Saver()\n","  saver.restore(sess, tf.train.latest_checkpoint('drive/checkpoints/'))\n","  while True:\n","    inp = input()\n","    st = sentence_to_seq(inp, source_vocab_to_int)\n","    trans_logits = sess.run(inference_logits, feed_dict={input_data: [st]*batch_size,\n","                                           target_sequence_length: [len(st)*2]*batch_size,\n","                                           source_sequence_length: [len(st)*2]*batch_size,\n","                                           keep_prob: 1.0})[0]\n","    print('Input')\n","    print('  Word Ids:      {}'.format([i for i in st]))\n","    print('  input : {}'.format([source_int_to_vocab[i] for i in st]))\n","\n","    print('\\nPrediction')\n","    print('  Word Ids:      {}'.format([i for i in trans_logits]))\n","    print('  reply: {}'.format(\" \".join([target_int_to_vocab[i] for i in trans_logits])))\n","\n","\n","\n","\n","\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from drive/checkpoints/dev-13\n","she is a bitch\n","Input\n","  Word Ids:      [4046, 2356, 6183, 964]\n","  input : ['she', 'is', 'a', 'bitch']\n","\n","Prediction\n","  Word Ids:      [4046, 2356, 6183, 8095, 8094, 0, 0, 0]\n","  reply: she is a <UNK> <EOS> greek greek greek\n","\n","Input\n","  Word Ids:      [8095]\n","  input : ['<UNK>']\n","\n","Prediction\n","  Word Ids:      [2362, 4643]\n","  reply: whew typical\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \"\"\"\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-02fb444dfdc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/checkpoints/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m     \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_vocab_to_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     trans_logits = sess.run(inference_logits, feed_dict={input_data: [st]*batch_size,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"l7XNFALEocT8","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"LGr43buvk9Jx","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}